{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import *\n",
    "from pytorch_util import *\n",
    "from pytorch_models import ConvBatchLeaky1D,GRU_NCL,LambdaLayer,biasLayer,SincBatchLeaky\n",
    "from torch.nn import Linear,Sequential,LeakyReLU,Dropout,L1Loss,LayerNorm,BatchNorm1d\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change ##\n",
    "batch_size = 16\n",
    "clip = .1\n",
    "block = ResidualBlock\n",
    "normalFun = normalize2\n",
    "name = 'raw_sincNet_attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/train.csv\", \n",
    "                    dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})\n",
    "train = train.values\n",
    "train[:,0] = normalFun(train[:,0])\n",
    "# without tanh is worse-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gen = SequenceGen(train)\n",
    "# train_gen = DataLoader(train_gen,batch_size,False,num_workers=2)\n",
    "\n",
    "# val_gen = SequenceGen(train,False)\n",
    "# val_gen = DataLoader(val_gen,batch_size,False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = SequenceGenSample(train)\n",
    "train_gen = DataLoader(train_gen,batch_size,False,num_workers=2)\n",
    "\n",
    "val_gen = SequenceGenSample(train,False)\n",
    "val_gen = DataLoader(val_gen,batch_size,False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 167])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadedAttention(2,size,dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(size,size,dropout)\n",
    "        self.sublayer = nn.ModuleList([SublayerConnection(size, dropout),SublayerConnection(size, dropout)])\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(   LayerNorm(37500),\n",
    "                      Dropout(0.1),\n",
    "                      SincBatchLeaky(1,128,256,sample_rate=1000000,stride=128),\n",
    "                      Dropout(0.1),\n",
    "                      LambdaLayer(lambda x:x.transpose(1,2)),\n",
    "                      EncoderLayer(128,0.5),\n",
    "                      Linear(128,1,bias=False),\n",
    "                      Dropout(0.1),                 \n",
    "                      LambdaLayer(lambda x:x.squeeze(2).mean(1,keepdim=True)),                 \n",
    "                      biasLayer((1,)),\n",
    "                      LambdaLayer(lambda x:x.squeeze(1))\n",
    "                      ).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(trainable_parameter(model),lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_nor = loss_func_generator(L1Loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:2.68786307825492, val_loss:2.562455177307129\n",
      "epoch:1, train_loss:2.2781395877783117, val_loss:2.5355451107025146\n",
      "epoch:2, train_loss:2.2009986696334987, val_loss:2.476217031478882\n",
      "epoch:3, train_loss:2.1581294926313253, val_loss:2.4494194984436035\n",
      "epoch:4, train_loss:2.1159924664176426, val_loss:2.496201276779175\n",
      "Training completed in 15.810860872268677s\n"
     ]
    }
   ],
   "source": [
    "model = fit(5, model, loss_func_nor, opt, train_gen, val_gen,clip=clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission_sample(name,model,normalFun,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
